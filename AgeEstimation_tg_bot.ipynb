{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0584eb9c2776402783246bb4829696a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9501803d2aa648b782dc49e53f34bb81",
              "IPY_MODEL_88c4da27f44b442a812f68fdb6821fd6",
              "IPY_MODEL_67b28df832ad4eedacf8938213159f98"
            ],
            "layout": "IPY_MODEL_8ee99f5508214eaf81c908b3b14d85b4"
          }
        },
        "9501803d2aa648b782dc49e53f34bb81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed6edef7ad5f4521a6b66cb6c5f28941",
            "placeholder": "​",
            "style": "IPY_MODEL_5f9195d975e046ea9dc054e811bdfdcd",
            "value": "100%"
          }
        },
        "88c4da27f44b442a812f68fdb6821fd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03780cc011bd4d0785eef4fb92ec8c64",
            "max": 87306240,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_caeec27f4f504782ae09cd898ae586b6",
            "value": 87306240
          }
        },
        "67b28df832ad4eedacf8938213159f98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_155331edc0c64e8aaea4217814aa0618",
            "placeholder": "​",
            "style": "IPY_MODEL_775c9c5960ed4f9480a47ecfe08f3653",
            "value": " 83.3M/83.3M [00:01&lt;00:00, 84.4MB/s]"
          }
        },
        "8ee99f5508214eaf81c908b3b14d85b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed6edef7ad5f4521a6b66cb6c5f28941": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f9195d975e046ea9dc054e811bdfdcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03780cc011bd4d0785eef4fb92ec8c64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "caeec27f4f504782ae09cd898ae586b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "155331edc0c64e8aaea4217814aa0618": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "775c9c5960ed4f9480a47ecfe08f3653": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uAs-n4K0JKB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facenet_pytorch"
      ],
      "metadata": {
        "id": "zHb-AkUR-5t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytelegrambotapi"
      ],
      "metadata": {
        "id": "dxCWsHBsEaA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import csv\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from scipy.spatial.distance import euclidean\n",
        "from facenet_pytorch import MTCNN\n",
        "import math\n",
        "import cv2\n",
        "import PIL.Image\n",
        "import functools\n",
        "import telebot\n",
        "from telebot.types import InputMediaPhoto\n",
        "import shutil\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "cW6oeYveH3O-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qElZU86fX-Xj",
        "outputId": "8fc715cd-b2ef-492d-a126-5a7ea4d54fe3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "facenet-pytorch==2.5.2\n",
            "torch @ https://download.pytorch.org/whl/cu113/torch-1.12.1%2Bcu113-cp38-cp38-linux_x86_64.whl\n",
            "torchaudio @ https://download.pytorch.org/whl/cu113/torchaudio-0.12.1%2Bcu113-cp38-cp38-linux_x86_64.whl\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.13.1\n",
            "torchvision @ https://download.pytorch.org/whl/cu113/torchvision-0.13.1%2Bcu113-cp38-cp38-linux_x86_64.whl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "def cosine_distance(x, y):\n",
        "    if x.ndim == 1:\n",
        "        x_norm = np.linalg.norm(x)\n",
        "        y_norm = np.linalg.norm(y)\n",
        "    elif x.ndim == 2:\n",
        "        x_norm = np.linalg.norm(x, axis=1, keepdims=True)\n",
        "        y_norm = np.linalg.norm(y, axis=1, keepdims=True)\n",
        "\n",
        "    np.seterr(divide='ignore', invalid='ignore')\n",
        "    s = np.dot(x, y.T)/(x_norm*y_norm)\n",
        "    s *= -1\n",
        "    dist = s + 1\n",
        "    dist = np.clip(dist, 0, 2)\n",
        "    if x is y or y is None:\n",
        "        dist[np.diag_indices_from(dist)] = 0.0\n",
        "    if np.any(np.isnan(dist)):\n",
        "        if x.ndim == 1:\n",
        "            dist = 1.\n",
        "        else:\n",
        "            dist[np.isnan(dist)] = 1.\n",
        "    return dist"
      ],
      "metadata": {
        "id": "5IKwUsmlSBfr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "4BqoCgTDSdX2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "v1C1P_ogSiVN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "\n",
        "        # define margin cnn\n",
        "        self.conv1_d = nn.Conv1d(101, 64, kernel_size=7, stride=2, padding=3, bias=False)  # 614/2 64\n",
        "        self.bn1_d = nn.BatchNorm1d(64)\n",
        "        # 150 64\n",
        "        self.layer1_d = nn.Sequential(\n",
        "            nn.MaxPool1d(kernel_size=3, stride=2, padding=1),\n",
        "            nn.Conv1d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # output 75 128\n",
        "        self.layer2_d = nn.Sequential(\n",
        "            nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1),  # conv replacing pooling\n",
        "            nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # output 37 256\n",
        "        self.layer3_d = nn.Sequential(\n",
        "            nn.Conv1d(128, 256, kernel_size=3, stride=2, padding=1),  # conv replacing pooling\n",
        "            nn.Conv1d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # output 20 512\n",
        "        self.layer4_d = nn.Sequential(\n",
        "            nn.Conv1d(256, 512, kernel_size=3, stride=2, padding=1),  # conv replacing pooling\n",
        "            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.avgpool_d = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc_d = nn.Linear(512, 101)  # the number of margin_l=101\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, nn.Conv1d)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm, nn.BatchNorm1d)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def margin_long_tail(self, x):\n",
        "        x = self.conv1_d(x)\n",
        "        x = self.bn1_d(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer1_d(x)\n",
        "        x = self.layer2_d(x)\n",
        "        x = self.layer3_d(x)\n",
        "        x = self.layer4_d(x)\n",
        "        x = self.avgpool_d(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc_d(x)\n",
        "        return x\n",
        "\n",
        "    def margin_miu(self, x):\n",
        "        x = self.conv1_d(x)\n",
        "        x = self.bn1_d(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer1_d(x)\n",
        "        x = self.layer2_d(x)\n",
        "        x = self.layer3_d(x)\n",
        "        x = self.layer4_d(x)\n",
        "        x = self.avgpool_d(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc_d(x)\n",
        "        x = (x-torch.min(x))/(torch.max(x)-torch.min(x))*101\n",
        "        return x\n",
        "\n",
        "    def margin_sigma(self, x):\n",
        "        x = self.conv1_d(x)\n",
        "        x = self.bn1_d(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer1_d(x)\n",
        "        x = self.layer2_d(x)\n",
        "        x = self.layer3_d(x)\n",
        "        x = self.layer4_d(x)\n",
        "        x = self.avgpool_d(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc_d(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def gaussian(z, age, m_p_miu, m_p_sigma):  # Calculate the Gaussian distribution for each age\n",
        "        x = torch.linspace(0, 100, 101).expand([z.shape[0], -1]).cuda()\n",
        "        pi = torch.Tensor([3.1415926]).cuda()\n",
        "        u = m_p_miu.T[age]\n",
        "        sig = m_p_sigma.T[age]\n",
        "        m_p = torch.exp(-torch.pow((x - u), 2)/(2 * torch.pow(sig, 2))) / (torch.sqrt(2 * pi) * sig)\n",
        "        return m_p\n",
        "\n",
        "    @staticmethod\n",
        "    def distributed_softmax(x, margin):\n",
        "        a = torch.ones(x.shape[1]).cuda()\n",
        "        mask = torch.diag(a).cuda()  # 1D vector  Output a 2D square matrix with input as diagonal elements\n",
        "        mask = (1 - mask).expand((x.shape[0], -1, -1))  # diagonal is 0, the others are 1\n",
        "\n",
        "        b = x.expand([x.shape[1], -1, -1]).permute(1, 0, 2)  # batch_size, multi, score\n",
        "        b = b * mask\n",
        "        b = torch.sum(torch.exp(b), dim=2) - 1  # eliminate exp(0) sum of the negative score\n",
        "        y = torch.exp(x - margin) / (b + torch.exp(x - margin))\n",
        "        return y\n",
        "\n",
        "    def _forward_impl(self, x, age, pro, intra, inter):\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        # propotype update\n",
        "        z = x.cpu().clone().detach().numpy()\n",
        "        age = age.cpu().clone().detach().numpy()\n",
        "        pro_t = pro[0].copy()\n",
        "        intra_t = intra.copy()\n",
        "        inter_t = inter.copy()\n",
        "        for i in range(z.shape[0]):\n",
        "            temp = pro[0][age[i], :].copy()  # shallow copy\n",
        "            pro[0][age[i]] = pro[0][age[i], :] + (z[i] - pro[0][age[i], :]) / (pro[1][age[i]] + 1)  # update Prototype\n",
        "            pro[1][age[i]] += 1  # update instance number\n",
        "            # cosine_distances 0~2 Divisor 0 is set to 1 for unknown relationship\n",
        "            intra[age[i]] = intra[age[i]] + cosine_distance(z[i], temp) * cosine_distance(z[i], pro[0][age[i]])\n",
        "        # Calculate distance between two matrices and multi-row vectors\n",
        "        inter = cosine_distance(pro[0], pro[0])\n",
        "        delta_pro = np.concatenate((pro[0]-pro_t, intra-intra_t, inter-inter_t), axis=1)[np.newaxis, :]\n",
        "        pro_input = np.concatenate((pro[0], intra), axis=1)[np.newaxis, :]\n",
        "        m_l = self.margin_long_tail(torch.from_numpy(delta_pro).cuda())\n",
        "        m_p_miu = self.margin_miu(torch.from_numpy(pro_input).cuda())\n",
        "        m_p_sigma = self.margin_sigma(torch.from_numpy(pro_input).cuda())\n",
        "        m_p = self.gaussian(z, age, m_p_miu, m_p_sigma)\n",
        "        margin = m_p + 0.1*m_l\n",
        "        # The margin is normalized to 0 as the mean and 1 as the variance\n",
        "        margin = (margin-torch.mean(margin, dim=1).expand([101, -1]).permute(1, 0))/torch.std(margin, dim=1).expand([101, -1]).permute(1, 0)\n",
        "        margin = margin*0.01\n",
        "        if not False: #cfg.model.margin: эта штука False у них стоит\n",
        "            margin = margin*0\n",
        "        x = self.fc(x)\n",
        "        if margin.requires_grad:\n",
        "            x = F.softmax(x-margin, dim=1)  # make a baseline\n",
        "            # x = F.softmax(x-margin, dim=1)\n",
        "        else:\n",
        "            x =F.softmax(x, dim=1)\n",
        "        # L1 normalize\n",
        "        # x = F.normalize(x-margin, p=1, dim=1)  # negative logarithm possible\n",
        "        return x, pro, intra, inter\n",
        "\n",
        "    def forward(self, x, age, proc, intra, inter):\n",
        "        return self._forward_impl(x, age, proc, intra, inter)\n"
      ],
      "metadata": {
        "id": "7mjd43uFRo1M"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.hub import load_state_dict_from_url\n",
        "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
        "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
        "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
        "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
        "}\n",
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        # partial load pretrained model\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(model_dict)\n",
        "    return model"
      ],
      "metadata": {
        "id": "0l0uHoa2V31I"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet34(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-34 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,\n",
        "                   **kwargs)"
      ],
      "metadata": {
        "id": "mHBFt0ofWd-f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(img):\n",
        "  img = Image.open(img).convert('RGB')\n",
        "  imgs = [img, img.transpose(Image.FLIP_LEFT_RIGHT)]\n",
        "  transform_list = [\n",
        "      transforms.Resize((224, 224), interpolation=3),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "  ]\n",
        "  transform = transforms.Compose(transform_list)\n",
        "  imgs = [transform(i) for i in imgs]\n",
        "  imgs = [torch.unsqueeze(i, dim=0) for i in imgs]\n",
        "\n",
        "  return imgs"
      ],
      "metadata": {
        "id": "b6sIMFoZIeeV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet34(pretrained=True, progress=True)\n",
        "fc_in_features = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(fc_in_features, 101)"
      ],
      "metadata": {
        "id": "cX5wvA3TMQjz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "0584eb9c2776402783246bb4829696a0",
            "9501803d2aa648b782dc49e53f34bb81",
            "88c4da27f44b442a812f68fdb6821fd6",
            "67b28df832ad4eedacf8938213159f98",
            "8ee99f5508214eaf81c908b3b14d85b4",
            "ed6edef7ad5f4521a6b66cb6c5f28941",
            "5f9195d975e046ea9dc054e811bdfdcd",
            "03780cc011bd4d0785eef4fb92ec8c64",
            "caeec27f4f504782ae09cd898ae586b6",
            "155331edc0c64e8aaea4217814aa0618",
            "775c9c5960ed4f9480a47ecfe08f3653"
          ]
        },
        "outputId": "6f2f36fa-6cb6-433c-db7d-95e9d174034e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/83.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0584eb9c2776402783246bb4829696a0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "mtcnn = MTCNN(post_process=False, device=device)"
      ],
      "metadata": {
        "id": "0NfyXGtf-1in"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfnSKziLYCYj",
        "outputId": "63e0b5b6-8e5b-4738-dfb7-231a462e4029"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = torch.load('/content/drive/MyDrive/AgeEstimation/best.pth')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "JeKhLRBJROPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# state = torch.load('/content/best.pth')\n",
        "model.load_state_dict(state)\n",
        "# model = model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "vm5LVSeRMSz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def alignment_procedure(img, left_eye, right_eye):\n",
        "    #this function aligns given face in img based on left and right eye coordinates\n",
        "    \n",
        "    left_eye_x, left_eye_y = left_eye\n",
        "    right_eye_x, right_eye_y = right_eye\n",
        "    \n",
        "    #-----------------------\n",
        "    #find rotation direction\n",
        "    \n",
        "    if left_eye_y > right_eye_y:\n",
        "        point_3rd = (right_eye_x, left_eye_y)\n",
        "        direction = -1 #rotate same direction to clock\n",
        "    else:\n",
        "        point_3rd = (left_eye_x, right_eye_y)\n",
        "        direction = 1 #rotate inverse direction of clock\n",
        "    \n",
        "    #-----------------------\n",
        "    #find length of triangle edges\n",
        "    \n",
        "    a = euclidean(left_eye, point_3rd)\n",
        "    b = euclidean(right_eye, point_3rd)\n",
        "    c = euclidean(right_eye, left_eye)\n",
        "    \n",
        "    #-----------------------\n",
        "    \n",
        "    #apply cosine rule\n",
        "    \n",
        "    if b != 0 and c != 0: #this multiplication causes division by zero in cos_a calculation\n",
        "    \n",
        "        cos_a = (b*b + c*c - a*a)/(2*b*c)\n",
        "        angle = np.arccos(cos_a) #angle in radian\n",
        "        angle = (angle * 180) / math.pi #radian to degree\n",
        "    \n",
        "        #-----------------------\n",
        "        #rotate base image\n",
        "    \n",
        "        if direction == -1:\n",
        "            angle = 90 - angle\n",
        "    \n",
        "        img = Image.fromarray(img)\n",
        "        img = np.array(img.rotate(direction * angle))\n",
        "    \n",
        "    #-----------------------\n",
        "    \n",
        "    return img #return img anyway"
      ],
      "metadata": {
        "id": "GrsOyGli-SIZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_face(input_path):\n",
        "    img = cv2.imread(input_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    bbox, _, landmarks = mtcnn.detect(img, landmarks=True)\n",
        "\n",
        "    if bbox is None:\n",
        "        # self.non_detect.append(self.path + img_path)\n",
        "        return \"can't detect\"\n",
        "    if landmarks is None:\n",
        "        # self.non_align.append(self.path + img_path)\n",
        "        return \"can't detect\"\n",
        "\n",
        "\n",
        "    bbox = list(map(int, bbox[0]))\n",
        "    bbox = [max(0, int(x)) for x in bbox]\n",
        "    img = img[bbox[1]: bbox[3], bbox[0]: bbox[2], :]\n",
        "    align = alignment_procedure(img, (int(landmarks[0][0][0]),\n",
        "                    int(landmarks[0][0][1])),\n",
        "                    (int(landmarks[0][1][0]), \n",
        "                    int(landmarks[0][1][1])))\n",
        "    \n",
        "    return align, img"
      ],
      "metadata": {
        "id": "xg4FQWuj_Ee6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(img_path, uid):\n",
        "    \n",
        "    rank = torch.Tensor([i for i in range(101)]).cuda()\n",
        "\n",
        "    age = 20\n",
        "    age = torch.IntTensor([int(age)])\n",
        "    age = age.to(device)\n",
        "\n",
        "    p = detect_face(img_path)\n",
        "\n",
        "    if type(p) == str:\n",
        "        return \"MTCNN can't detect face\"\n",
        "    else:\n",
        "        align, not_align = p\n",
        "        align = cv2.cvtColor(align, cv2.COLOR_BGR2RGB)\n",
        "        not_align = cv2.cvtColor(not_align, cv2.COLOR_BGR2RGB)\n",
        "        cv2.imwrite(str(uid) + 'align.jpg', align) \n",
        "        cv2.imwrite(str(uid) + 'not_align.jpg', not_align)\n",
        "        imgs = preprocess(str(uid) + 'align.jpg')\n",
        "        imgs2 = preprocess(str(uid) + 'not_align.jpg')\n",
        "        predict_age_align = 0\n",
        "        predict_age_not_align = 0\n",
        "        prototype = np.zeros([101, 512], dtype=np.float32)\n",
        "        instance_num = np.zeros([101, 1], dtype=np.float32)\n",
        "        intra = np.zeros([101, 1], dtype=np.float32)\n",
        "        inter = np.zeros([101, 101], dtype=np.float32)\n",
        "        pro = [prototype, instance_num]\n",
        "\n",
        "        for img in imgs:\n",
        "            img = img.to(device)\n",
        "            output, pro, intra, inter = model(img, age, pro, intra, inter)\n",
        "            predict_age_align += torch.sum(output * rank, dim=1).item() / 2\n",
        "\n",
        "        for img in imgs2:\n",
        "            img = img.to(device)\n",
        "            output, pro, intra, inter = model(img, age, pro, intra, inter)\n",
        "            predict_age_not_align += torch.sum(output * rank, dim=1).item() / 2\n",
        "\n",
        "    # print(predict_age_align)\n",
        "    # print(predict_age_not_align)\n",
        "\n",
        "    return predict_age_align, predict_age_not_align"
      ],
      "metadata": {
        "id": "Q5pfOdexLkyA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_photo(photo, uid, postfix):\n",
        "    fileID = photo.photo[-1].file_id\n",
        "\n",
        "    file_info = bot.get_file(fileID)\n",
        "\n",
        "    downloaded_file = bot.download_file(file_info.file_path)\n",
        "    path = '../' + str(uid) + postfix + \".jpg\"\n",
        "    with open(path, 'wb') as new_file:\n",
        "        new_file.write(downloaded_file)\n",
        "    return path#open(str(uid) + postfix + \".jpg\", 'rb')"
      ],
      "metadata": {
        "id": "GtuBaQyPEU9V"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bot = telebot.TeleBot('5984227231:AAFpG9jBO6QePoVf9suamH0O0cjLfhQpw08') # insert ur token here\n",
        "\n",
        "\n",
        "@bot.message_handler(commands=['start'])\n",
        "def start_message(message):\n",
        "    user_id = message.from_user.id\n",
        "    bot.send_message(message.chat.id, 'Привет! Тут можно оценить свой возраст\\nНапиши /help, если ты не знаешь как со мной работать.')\n",
        "\n",
        "\n",
        "@bot.message_handler(commands=['help'])\n",
        "def start_message(message):\n",
        "    user_id = message.from_user.id\n",
        "    bot.send_message(message.chat.id, 'Отправь мне фотографию, я оценю твой возраст! :)')\n",
        "\n",
        "\n",
        "\n",
        "@bot.message_handler(content_types=['photo'])\n",
        "def send_photo(photo):\n",
        "    user_id = photo.from_user.id\n",
        "\n",
        "    img_path = preprocess_photo(photo, user_id, 'age')\n",
        "\n",
        "    p = inference(img_path, user_id)\n",
        "    if type(p[0]) == str:\n",
        "        message = p\n",
        "    else:\n",
        "        p1, p2 = p\n",
        "        message = f'alignment face prediction: {p1}\\nnot alignment face prediction: {p2}'\n",
        "    \n",
        "\n",
        "    bot.send_message(photo.chat.id, message)\n",
        "\n",
        "\n",
        "\n",
        "bot.polling()"
      ],
      "metadata": {
        "id": "4jGl1X8TEgHN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pOddw4LRr6aT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}